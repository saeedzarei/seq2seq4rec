{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e42bc7",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"recsys_data.csv\")\n",
    "\n",
    "# if using reverst input/output\n",
    "df.rename(columns={\"input_seq\": \"target_seq\", \"target_seq\": \"input_seq\"}, inplace=True)\n",
    "\n",
    "# Keep only the desired columns\n",
    "df = df[['input_seq', 'target_seq']]\n",
    "\n",
    "df = df.dropna(subset=['input_seq', 'target_seq'])\n",
    "\n",
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_data = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# Combine into DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "train_data, valid_data = (\n",
    "    dataset_dict[\"train\"],\n",
    "    dataset_dict[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3053d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, max_length, sos_token, eos_token):\n",
    "    input_tokens = [token for token in example[\"input_seq\"].split()][:max_length]\n",
    "    target_tokens = [token for token in example[\"target_seq\"].split()][:max_length]\n",
    "\n",
    "    input_tokens = [sos_token] + input_tokens + [eos_token]\n",
    "    target_tokens = [sos_token] + target_tokens + [eos_token]\n",
    "    return {\"input_tokens\": input_tokens, \"target_tokens\": target_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"max_length\": max_length,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebf992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import tqdm\n",
    "\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "input_toks = [unk_token, pad_token, sos_token, eos_token]\n",
    "target_toks = [unk_token, pad_token, sos_token, eos_token]\n",
    "for row in df.itertuples(index=False):\n",
    "  target_seq = row.target_seq\n",
    "  input_seq = row.input_seq\n",
    "  for i in target_seq.split():\n",
    "    if i not in target_toks:\n",
    "      target_toks.append(i)\n",
    "  for i in input_seq.split():\n",
    "    if i not in input_toks:\n",
    "      input_toks.append(i)\n",
    "\n",
    "input_toks.sort()\n",
    "target_toks.sort()\n",
    "\n",
    "input_vocab = {tok:idx for idx, tok in enumerate(input_toks)}\n",
    "target_vocab = {tok:idx for idx, tok in enumerate(target_toks)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, input_vocab, target_vocab):\n",
    "    input_ids = [input_vocab[i] for i in example[\"input_tokens\"]]\n",
    "    target_ids = [target_vocab[i] for i in example[\"target_tokens\"]]\n",
    "    return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"input_vocab\": input_vocab, \"target_vocab\": target_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f748d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"input_ids\", \"target_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_input_ids = [example[\"input_ids\"] for example in batch]\n",
    "        batch_target_ids = [example[\"target_ids\"] for example in batch]\n",
    "        batch_input_ids = nn.utils.rnn.pad_sequence(batch_input_ids, padding_value=pad_index)\n",
    "        batch_target_ids = nn.utils.rnn.pad_sequence(batch_target_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"input_ids\": batch_input_ids,\n",
    "            \"target_ids\": batch_target_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "unk_index = input_vocab['<unk>']\n",
    "pad_index = input_vocab['<pad>']\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, input_vocab['<pad>'], shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, input_vocab['<pad>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e2de9",
   "metadata": {},
   "source": [
    "define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/bentrevett/pytorch-seq2seq\n",
    "'''\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.hidden_dim = hidden_dim\n",
    "      self.n_layers = n_layers\n",
    "      self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "      self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "      self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, input, hidden, cell):\n",
    "      input = input.unsqueeze(0)\n",
    "      embedded = self.dropout(self.embedding(input))\n",
    "      output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "      prediction = self.fc_out(output.squeeze(0))\n",
    "      return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a56b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(input_vocab)\n",
    "output_dim = len(target_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 768\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408b39b",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    loop = tqdm.tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    for i, batch in enumerate(loop):\n",
    "        src = batch[\"input_ids\"].to(device)\n",
    "        trg = batch[\"target_ids\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    loop = tqdm.tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loop):\n",
    "            src = batch[\"input_ids\"].to(device)\n",
    "            trg = batch[\"target_ids\"].to(device)\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for epoch in range(last_epoch, n_epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    #save last model\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch+1,\n",
    "            'loss': train_loss,\n",
    "        }, \"model/recsys_model_last_reversed.pt\")\n",
    "\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "\n",
    "#validate after two epoch\n",
    "valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "print(f\"Valid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
