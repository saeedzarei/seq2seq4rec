{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80f0ac9",
   "metadata": {},
   "source": [
    "load target client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09544846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "array = np.load('ubc/relevant_clients.npy')\n",
    "\n",
    "# Convert to a Python list\n",
    "client_ids_list = array.tolist()\n",
    "len(client_ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80d00e",
   "metadata": {},
   "source": [
    "create input and target vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9373d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title create input and target vocabs\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"recsys_data.csv\")\n",
    "\n",
    "# Keep only the desired columns\n",
    "df = df[['input_seq', 'target_seq']]\n",
    "\n",
    "df = df.dropna(subset=['input_seq', 'target_seq'])\n",
    "\n",
    "\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "input_toks = [unk_token, pad_token, sos_token, eos_token]\n",
    "target_toks = [unk_token, pad_token, sos_token, eos_token]\n",
    "for row in df.itertuples(index=False):\n",
    "  target_seq = row.target_seq\n",
    "  input_seq = row.input_seq\n",
    "  for i in target_seq.split():\n",
    "    if i not in target_toks:\n",
    "      target_toks.append(i)\n",
    "  for i in input_seq.split():\n",
    "    if i not in input_toks:\n",
    "      input_toks.append(i)\n",
    "\n",
    "input_toks.sort()\n",
    "target_toks.sort()\n",
    "\n",
    "input_vocab = {tok:idx for idx, tok in enumerate(input_toks)}\n",
    "target_vocab = {tok:idx for idx, tok in enumerate(target_toks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87494bb",
   "metadata": {},
   "source": [
    "initial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518dc802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title init models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.hidden_dim = hidden_dim\n",
    "      self.n_layers = n_layers\n",
    "      self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "      self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "      self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, input, hidden, cell):\n",
    "      input = input.unsqueeze(0)\n",
    "      embedded = self.dropout(self.embedding(input))\n",
    "      output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "      prediction = self.fc_out(output.squeeze(0))\n",
    "      return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 768\n",
    "encoder = Encoder(129, 256, hidden_size, 2, 0.5,)\n",
    "decoder = Decoder(189, 256, hidden_size, 2, 0.5,)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "encoder = Encoder(189, 256, hidden_size, 2, 0.5,)\n",
    "decoder = Decoder(129, 256, hidden_size, 2, 0.5,)\n",
    "model_revesed = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"recsys_model_last.pt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"model recsys_model_last epoch: {checkpoint['epoch']}\")\n",
    "checkpoint = torch.load(\"recsys_model_last_reversed.pt\", map_location=torch.device('cpu'))\n",
    "model_revesed.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"model recsys_model_last_reversed epoch: {checkpoint['epoch']}\")\n",
    "model.to(device)\n",
    "model_revesed.to(device)\n",
    "\n",
    "print(\"all models loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_rdata = {}\n",
    "rdata = open(\"recsys_data.csv\").readlines()[1:]\n",
    "for i in rdata:\n",
    "  c_id = int(i.split(\",\")[0].strip())\n",
    "  dict_rdata[c_id] = i\n",
    "\n",
    "all_embeddings1 = np.zeros((1_000_000, 768), dtype=np.float32) #model A\n",
    "all_embeddings2 = np.zeros((1_000_000, 768), dtype=np.float32) #model B\n",
    "\n",
    "for idx, id in enumerate(client_ids_list):\n",
    "  if (idx+1)%50_000 == 0:\n",
    "    print(idx+1)\n",
    "\n",
    "  if id not in dict_rdata:\n",
    "    continue\n",
    "\n",
    "  input_seq = dict_rdata[id].split(\",\")[1].strip().strip(\"\\n\")\n",
    "  target_seq = dict_rdata[id].split(\",\")[2].strip().strip(\"\\n\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    #input\n",
    "    tokens = input_seq.strip().strip(\"\\n\").split()\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    ids = [input_vocab[i] for i in tokens]\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "    hidden, cell = model.encoder(tensor)\n",
    "    hidden = hidden.squeeze((0, 1))\n",
    "    mean_vec = (hidden[0] + hidden[1])/2\n",
    "    embed_in = mean_vec.detach().cpu().numpy()\n",
    "\n",
    "    #target\n",
    "    tokens = target_seq.strip().strip(\"\\n\").split()\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    ids = [target_vocab[i] for i in tokens]\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "    hidden, cell = model_revesed.encoder(tensor)\n",
    "    hidden = hidden.squeeze((0, 1))\n",
    "    mean_vec = (hidden[0] + hidden[1])/2\n",
    "    embed_tar = mean_vec.detach().cpu().numpy()\n",
    "    all_embeddings1[idx] = embed_in\n",
    "    all_embeddings2[idx] = embed_tar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537a3b3",
   "metadata": {},
   "source": [
    "Average two model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = (all_embeddings1 + all_embeddings2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26159fe1",
   "metadata": {},
   "source": [
    "create final submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_embeddings_fp16 = all_embeddings.astype(np.float16)\n",
    "np.save('embeddings.npy', all_embeddings_fp16)\n",
    "mv relevant_clients.npy client_ids.npy\n",
    "zip -r embeddings.zip embeddings.npy client_ids.npy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
