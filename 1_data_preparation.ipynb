{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a6b4b5",
   "metadata": {},
   "source": [
    "Merge all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52096a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define your files and associated type for the third column\n",
    "file_map = {\n",
    "    \"ubc_data/page_visit.parquet\": \"url\",\n",
    "    \"ubc_data/search_query.parquet\": \"query\",\n",
    "    \"ubc_data/add_to_cart.parquet\": \"sku\",\n",
    "    \"ubc_data/product_buy.parquet\": \"sku\",\n",
    "    \"ubc_data/remove_from_cart.parquet\": \"sku\",\n",
    "\n",
    "}\n",
    "# List to hold DataFrames\n",
    "dfs = []\n",
    "\n",
    "for file_name, value_type in file_map.items():\n",
    "    try:\n",
    "        df = pd.read_parquet(file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure required columns\n",
    "    if not {\"client_id\", \"timestamp\"}.issubset(df.columns):\n",
    "        print(f\"Skipping {file_name}: missing client_id or timestamp\")\n",
    "        continue\n",
    "\n",
    "    # Find third column (value column)\n",
    "    value_cols = [col for col in df.columns if col not in {\"client_id\", \"timestamp\"}]\n",
    "    if len(value_cols) != 1:\n",
    "        print(f\"Skipping {file_name}: expected 1 value column, found {len(value_cols)}\")\n",
    "        continue\n",
    "\n",
    "    value_col = value_cols[0]\n",
    "\n",
    "    # Prepare a unified DataFrame\n",
    "    tmp = df[[\"client_id\", \"timestamp\", value_col]].copy()\n",
    "    tmp = tmp.rename(columns={value_col: \"value\"})\n",
    "    tmp[\"type\"] = value_type\n",
    "    tmp[\"source\"] = os.path.basename(file_name)\n",
    "\n",
    "    # Convert value to string to avoid mixed-type Parquet errors\n",
    "    tmp[\"value\"] = tmp[\"value\"].astype(str)\n",
    "\n",
    "    # Reorder columns\n",
    "    tmp = tmp[[\"client_id\", \"timestamp\", \"source\", \"type\", \"value\"]]\n",
    "\n",
    "    dfs.append(tmp)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save to Parquet\n",
    "merged_df.to_parquet(\"merged_behavior.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff1b15",
   "metadata": {},
   "source": [
    "load merged df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae26822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the parquet file\n",
    "merged_behavior_df = pd.read_parquet(\"merged_behavior.parquet\")\n",
    "print(merged_behavior_df.shape)\n",
    "merged_behavior_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe882443",
   "metadata": {},
   "source": [
    "create sequnce of urls that end with a sku, in define session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sku_to_urls = defaultdict(list)\n",
    "current_client_id = 0\n",
    "session_urls = []\n",
    "current_timestamp = \"\"\n",
    "delta = timedelta(minutes=30)\n",
    "\n",
    "for idx, row in enumerate(merged_behavior_df.itertuples(index=False)):\n",
    "    if (idx+1) %10_000_000 == 0:\n",
    "        print(f\"processed {idx+1} items.\")\n",
    "\n",
    "    client_id = row.client_id\n",
    "    timestamp = row.timestamp\n",
    "    event_type = row.type\n",
    "    value = row.value\n",
    "    \n",
    "    if client_id != current_client_id:\n",
    "        current_client_id = client_id\n",
    "        session_urls = []\n",
    "    \n",
    "    if event_type == \"url\":\n",
    "        #remove duplicate url that occure in sequnce in similar timestamp\n",
    "        if len(session_urls) > 0:\n",
    "            if session_urls[-1] != (value, str(timestamp)):\n",
    "                session_urls.append((value, str(timestamp)))\n",
    "        else:\n",
    "            session_urls.append((value, str(timestamp)))\n",
    "    \n",
    "    elif event_type == \"sku\":\n",
    "        anchor_str = str(timestamp)\n",
    "        anchor = datetime.strptime(anchor_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if len(session_urls) == 0:\n",
    "            continue\n",
    "\n",
    "        elif abs(datetime.strptime(session_urls[0][1], \"%Y-%m-%d %H:%M:%S\") - anchor) <= delta:\n",
    "            sku_to_urls[value].append([i[0] for i in session_urls])\n",
    "        else:\n",
    "            filtered = [\n",
    "                    ts[0] for ts in session_urls\n",
    "                    if abs(datetime.strptime(ts[1], \"%Y-%m-%d %H:%M:%S\") - anchor) <= delta\n",
    "                ]\n",
    "            sku_to_urls[value].append(filtered)\n",
    "\n",
    "        session_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the parquet file\n",
    "product_properties_df = pd.read_parquet(\"ubc_data/product_properties.parquet\")\n",
    "print(product_properties_df.shape)\n",
    "product_properties_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c83874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sku_name = {}\n",
    "names = []\n",
    "for idx, row in enumerate(product_properties_df.itertuples(index=False)):\n",
    "\n",
    "    sku = row.sku\n",
    "    name = row.name\n",
    "    \n",
    "    name_int = list(map(int, name.strip().strip(\"[]\").split()))\n",
    "    sku_name[str(sku)] = str(name_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614912d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_str = [sku_name[i] for i in sku_name]\n",
    "names_str = list(set(names_str))\n",
    "names = [list(map(int,i.strip(\"]\").strip(\"[\").split(\",\"))) for i in names_str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd55228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "best_cluster = 60\n",
    "kmeans = KMeans(n_clusters=best_cluster, random_state=42, n_init='auto')\n",
    "labels = kmeans.fit_predict(names)\n",
    "name_cluster = {str(q):label  for q, label in zip(names, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_cluster = {}\n",
    "\n",
    "for i in sku_name:\n",
    "    sku_cluster[i] = name_cluster[sku_name[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3634ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "url_to_sku_cluster = defaultdict(list)\n",
    "for sku in sku_to_urls:\n",
    "    for urls in sku_to_urls[sku]:\n",
    "        for url in urls:\n",
    "            url_to_sku_cluster[url].append(sku_cluster[sku])\n",
    "\n",
    "url_cluster = {}\n",
    "for url in url_to_sku_cluster:\n",
    "    counter = Counter(url_to_sku_cluster[url])\n",
    "    most_common_value = counter.most_common(1)[0][0]\n",
    "    url_cluster[url] = most_common_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ef9ad",
   "metadata": {},
   "source": [
    "Query cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_raw = []\n",
    "\n",
    "\n",
    "for idx, row in enumerate(merged_behavior_df.itertuples(index=False)):\n",
    "    if (idx+1) %10_000_000 == 0:\n",
    "        print(idx+1)\n",
    "        with open(f\"query_{idx+1}.txt\", 'w') as q:\n",
    "            for i in queries_raw:\n",
    "                q.write(i + \"\\n\")\n",
    "\n",
    "        queries_raw = []\n",
    "\n",
    "    event_type = row.type\n",
    "    value = row.value\n",
    "    \n",
    "    if event_type == \"query\":\n",
    "        queries_raw.append(value)\n",
    "\n",
    "with open(f\"query_last.txt\", 'w') as q:\n",
    "    for i in queries_raw:\n",
    "        q.write(i + \"\\n\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366da68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "queries_raw = []\n",
    "query_files = glob.glob(\"query_*.txt\")\n",
    "\n",
    "for f in query_files:\n",
    "    print(f)\n",
    "    ln = open(f).readlines()\n",
    "    for l in ln:\n",
    "        l = l.strip(\"\\n\")\n",
    "        queries_raw.append(l)\n",
    "\n",
    "print(len(queries_raw))\n",
    "queries_raw = list(set(queries_raw))\n",
    "print(len(queries_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for q in queries_raw:\n",
    "    int_list = list(map(int, q.strip().strip(\"[]\").split()))\n",
    "    queries.append(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_cluster, random_state=42, n_init='auto')\n",
    "labels = kmeans.fit_predict(queries)\n",
    "query_cluster = {str(q):label  for q, label in zip(queries, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cluster = {\n",
    "    \"url\": url_cluster,\n",
    "    \"query\": query_cluster,\n",
    "    \"sku\": sku_cluster\n",
    "}\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"data_clusters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(combined_cluster, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7659c9b",
   "metadata": {},
   "source": [
    "Create Sequence data for feed to seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a21893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data_clusters.pkl\", \"rb\") as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595aafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "data = []\n",
    "\n",
    "seq_input = []\n",
    "seq_target = []\n",
    "\n",
    "max_len_thereshold = 500\n",
    "\n",
    "for idx, row in enumerate(merged_behavior_df.itertuples(index=False)):\n",
    "    if (idx+1) %10_000_000 == 0:\n",
    "        print(f\"processed {idx+1} items.\")\n",
    "        with open(f'sequnce_data_{idx+1}.csv', 'w') as sq:\n",
    "            for i in data:\n",
    "                sq.write(f\"{i[0]}#{\"_\".join(list(map(str, i[1])))}#{\"_\".join(list(map(str, i[2])))}\\n\")\n",
    "        data = []\n",
    "        # break\n",
    "    \n",
    "    # extract row items\n",
    "    client_id = row.client_id\n",
    "    timestamp = row.timestamp\n",
    "    source = row.source\n",
    "    event_type = row.type\n",
    "    value = row.value\n",
    "\n",
    "\n",
    "    # define variable in first loop\n",
    "    if idx == 0:\n",
    "        current_client_id = client_id\n",
    "        current_timestamp = timestamp\n",
    "\n",
    "    # if user changed or seq len was long\n",
    "    if current_client_id != client_id or len(seq_input) > max_len_thereshold:\n",
    "        # if len(seq_input) > 0 and len(seq_target) > 0:\n",
    "        data.append((current_client_id, seq_input, seq_target))\n",
    "\n",
    "        \n",
    "        current_client_id = client_id\n",
    "        current_timestamp = timestamp\n",
    "        seq_input = []\n",
    "        seq_target = []\n",
    "\n",
    "\n",
    "    time_diff = int((timestamp - current_timestamp).total_seconds())\n",
    "    \n",
    "    if source.split(\".\")[0] in [\"page_visit\", 'search_query']:\n",
    "        if len(seq_input) > 0  and  isinstance(seq_input[-1], int):\n",
    "            seq_input[-1] += time_diff\n",
    "            \n",
    "        else:\n",
    "            seq_input.append(time_diff)\n",
    "        \n",
    "        if source.split(\".\")[0] == \"page_visit\":\n",
    "            action = \"v\"\n",
    "        else:\n",
    "            action = \"s\"\n",
    "            value = list(map(int, value.strip().strip(\"[]\").split()))\n",
    "            # value = sum(value)/16\n",
    "\n",
    "            \n",
    "        seq_input.append(action + str(value))\n",
    "\n",
    "    else:\n",
    "        if len(seq_target) > 0  and  isinstance(seq_target[-1], int):\n",
    "            seq_target[-1] += time_diff\n",
    "            \n",
    "        else:\n",
    "            seq_target.append(time_diff)\n",
    "\n",
    "        if source.split(\".\")[0] == \"product_buy\":\n",
    "            action = \"b\"\n",
    "\n",
    "        elif source.split(\".\")[0] == \"add_to_cart\":\n",
    "            action = \"a\"\n",
    "\n",
    "        else:\n",
    "            action = \"r\"\n",
    "\n",
    "        seq_target.append(action + str(value))\n",
    "\n",
    "\n",
    "    current_timestamp = timestamp\n",
    "\n",
    "\n",
    "with open(f'sequnce_data_last.csv', 'w') as sq:\n",
    "    for i in data:\n",
    "        sq.write(f\"{i[0]}#{\"_\".join(list(map(str, i[1])))}#{\"_\".join(list(map(str, i[2])))}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59db003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "seq_files = glob.glob(\"sequnce_data_*.csv\")\n",
    "\n",
    "with open(\"seq2seq_raw_data.csv\", \"w\") as f:\n",
    "    for sf in seq_files:\n",
    "        print(sf)\n",
    "        data = open(sf).readlines()\n",
    "        for l in data:\n",
    "            f.write(l.strip(\"\\n\")+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"seq2seq_raw_data.csv\").readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61a220",
   "metadata": {},
   "source": [
    "duration cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "\n",
    "for l in data:\n",
    "    client_id = l.split(\"#\")[0].strip()\n",
    "    input_seq = l.split(\"#\")[1].strip()\n",
    "    output_seq = l.split(\"#\")[2].strip().strip(\"\\n\")\n",
    "    for tok in input_seq.split(\"_\"):\n",
    "        if tok.startswith(\"v\"): #page visit\n",
    "            pass\n",
    "        elif tok.startswith(\"s\"): #query search\n",
    "            pass\n",
    "        elif tok != '': # duration\n",
    "            durations.append(int(tok))\n",
    "\n",
    "    for tok in output_seq.split(\"_\"):\n",
    "        if tok.startswith(\"a\"): #add to cart\n",
    "            pass\n",
    "        elif tok.startswith(\"r\"): #remove from cart\n",
    "            pass\n",
    "        elif tok.startswith(\"b\"): #buy product\n",
    "            pass\n",
    "        elif tok != '': # duration\n",
    "            durations.append(int(tok))\n",
    "\n",
    "# filter duration\n",
    "durations = [i for i in durations if i>300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21388f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = np.array(durations).reshape(-1, 1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')\n",
    "labels = kmeans.fit_predict(X)\n",
    "durations_cluster = {str(d):label  for d, label in zip(durations, labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#save durations_cluster\n",
    "with open(\"durations_cluster.pkl\", \"wb\") as f:\n",
    "    pickle.dump(durations_cluster, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97715bd9",
   "metadata": {},
   "source": [
    "create cluster based sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#laod durations_cluster\n",
    "with open(\"durations_cluster.pkl\", \"rb\") as f:\n",
    "    durations_clusters = pickle.load(f)\n",
    "\n",
    "#laod loaded_dict\n",
    "with open(\"data_clusters.pkl\", \"rb\") as f:\n",
    "    loaded_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_data = []\n",
    "\n",
    "# data = open(\"seq2seq_raw_data.csv\").readlines()\n",
    "for l in data:\n",
    "    cdata = []\n",
    "    client_id = l.split(\"#\")[0].strip()\n",
    "    input_seq = l.split(\"#\")[1].strip()\n",
    "    output_seq = l.split(\"#\")[2].strip().strip(\"\\n\")\n",
    "    cdata.append(client_id)\n",
    "\n",
    "    cdata.append(\",\")\n",
    "    for tok in input_seq.split(\"_\"):\n",
    "        if tok.startswith(\"v\"): #page visit\n",
    "            if tok[1:] in loaded_dict['url']:\n",
    "              # if cdata[-1] != f\"v{loaded_dict['url'][tok[1:]]}\":\n",
    "              cdata.append(f\"v{loaded_dict['url'][tok[1:]]}\")\n",
    "\n",
    "        elif tok.startswith(\"s\"): #query search\n",
    "            if tok[1:] in loaded_dict['query']:\n",
    "                # if cdata[-1] != f\"s{loaded_dict['query'][tok[1:]]}\":\n",
    "              cdata.append(f\"s{loaded_dict['query'][tok[1:]]}\")\n",
    "        else: # duration\n",
    "            if tok in durations_clusters:\n",
    "              # if cdata[-1] != f\"d{durations_cluster[tok]}\":\n",
    "              cdata.append(f\"d{durations_clusters[tok]}\")\n",
    "\n",
    "    cdata.append(\",\")\n",
    "    for tok in output_seq.split(\"_\"):\n",
    "        if tok.startswith(\"a\"): #add to cart\n",
    "            if tok[1:] in loaded_dict['sku']:\n",
    "              # if cdata[-1] != f\"a{loaded_dict['sku'][tok[1:]]}\":\n",
    "              cdata.append(f\"a{loaded_dict['sku'][tok[1:]]}\")\n",
    "\n",
    "        elif tok.startswith(\"r\"): #remove from cart\n",
    "            if tok[1:] in loaded_dict['sku']:\n",
    "              # if cdata[-1] != f\"r{loaded_dict['sku'][tok[1:]]}\":\n",
    "              cdata.append(f\"r{loaded_dict['sku'][tok[1:]]}\")\n",
    "        elif tok.startswith(\"b\"): #buy product\n",
    "            if tok[1:] in loaded_dict['sku']:\n",
    "                # if cdata[-1] != f\"b{loaded_dict['sku'][tok[1:]]}\":\n",
    "              cdata.append(f\"b{loaded_dict['sku'][tok[1:]]}\")\n",
    "        else: # duration\n",
    "            if tok in durations_clusters:\n",
    "              # if cdata[-1] != f\"d{durations_cluster[tok]}\":\n",
    "              cdata.append(f\"d{durations_clusters[tok]}\")\n",
    "    # if len(\" \".join(cdata).replace(\" , \",\",\").split(\",\")[1].split()) > 1 and len(\" \".join(cdata).replace(\" , \",\",\").split(\",\")[2].split()) > 1:\n",
    "    seq2seq_data.append(\" \".join(cdata).replace(\" , \",\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec62153",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recsys_data.csv\", \"w\") as f:\n",
    "    f.write(\"client_id,input_seq,target_seq\\n\")\n",
    "    for l in seq2seq_data:\n",
    "        f.write(l+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
